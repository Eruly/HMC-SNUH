{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from HMSDatasets  import HMSHBACSpecDataset\n",
    "from model import HMSHBACSpecModel\n",
    "from utils.util import set_random_seed, to_device, get_path_label, get_transforms\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from torch import nn\n",
    "import glob\n",
    "from HMSDatasets import FullDataset\n",
    "import numpy as np\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "ori_cfg = omegaconf.OmegaConf.load(\"/home/ubuntu/HMC-SNUH/configs/0305.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "set_random_seed(ori_cfg.seed, deterministic=ori_cfg.deterministic)\n",
    "device = torch.device(ori_cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=10,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of eegs: 17089\n",
      "There are 11138 spectrogram parquets\n",
      "There are 17089 EEG spectrograms\n",
      "Shape of Kaggle spectrograms: (300, 400)\n",
      "Shape of EEG spectrograms: (128, 256, 4)\n",
      "Shape of raw eegs: (10000, 8)\n"
     ]
    }
   ],
   "source": [
    "train_all = pd.read_csv(\"fold_dataset/train_folds.csv\")\n",
    "\n",
    "if ori_cfg.dataset == \"Spectrogram\":\n",
    "    train_path_label, val_path_label, _, _ = get_path_label(cfg.val_fold, train_all)\n",
    "    train_transform, val_transform = get_transforms(cfg)\n",
    "    train_dataset = HMSHBACSpecDataset(**train_path_label, transform=train_transform)\n",
    "    val_dataset = HMSHBACSpecDataset(**val_path_label, transform=val_transform)\n",
    "elif ori_cfg.dataset == \"EEG\":\n",
    "    import omegaconf\n",
    "    cfg = omegaconf.OmegaConf.load(\"/home/ubuntu/HMC-SNUH/configs/datasets/init.yaml\")\n",
    "    TARGETS = [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\n",
    "    df = pd.read_csv(\"/data/SWB_Contribute/Data/original_data/train.csv\")\n",
    "    # Create a new identifier combining multiple columns\n",
    "    id_cols = ['eeg_id', 'spectrogram_id', 'seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "    df['new_id'] = df[id_cols].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    # Calculate the sum of votes for each class\n",
    "    df['sum_votes'] = df[TARGETS].sum(axis=1)\n",
    "    \n",
    "    # Group the data by the new identifier and aggregate various features\n",
    "    agg_functions = {\n",
    "        'eeg_id': 'first',\n",
    "        'eeg_label_offset_seconds': ['min', 'max'],\n",
    "        'spectrogram_label_offset_seconds': ['min', 'max'],\n",
    "        'spectrogram_id': 'first',\n",
    "        'patient_id': 'first',\n",
    "        'expert_consensus': 'first',\n",
    "        **{col: 'sum' for col in TARGETS},\n",
    "        'sum_votes': 'sum',\n",
    "    }\n",
    "    grouped_df = df.groupby('new_id').agg(agg_functions).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns and adjust column names\n",
    "    grouped_df.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in grouped_df.columns]\n",
    "    grouped_df.columns = grouped_df.columns.str.replace('_first', '').str.replace('_sum', '')\n",
    "    \n",
    "    # Normalize the class columns\n",
    "    y_data = grouped_df[TARGETS].values\n",
    "    y_data_normalized = y_data / y_data.sum(axis=1, keepdims=True)\n",
    "    grouped_df[TARGETS] = y_data_normalized\n",
    "\n",
    "    # Split the dataset into high and low quality based on the sum of votes\n",
    "    high_quality_df = grouped_df[grouped_df['sum_votes'] >= 10].reset_index(drop=True)\n",
    "    low_quality_df = grouped_df[(grouped_df['sum_votes'] < 10) & (grouped_df['sum_votes'] >= 0)].reset_index(drop=True)\n",
    "    train = grouped_df\n",
    "\n",
    "    # K fold\n",
    "    from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "    train = train.groupby(\"spectrogram_id\").head(1).reset_index(drop=True)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=ori_cfg.N_FOLDS, shuffle=True, random_state=ori_cfg.RANDOM_SEED)\n",
    "    train[\"fold\"] = -1\n",
    "\n",
    "    for fold_id, (_, val_idx) in enumerate(\n",
    "        sgkf.split(train, y=train[\"expert_consensus\"], groups=train[\"patient_id\"])\n",
    "    ):\n",
    "        train.loc[val_idx, \"fold\"] = fold_id\n",
    "    \n",
    "    train.to_csv('./fold_dataset/'+ \"train_folds.csv\", index=False)\n",
    "\n",
    "    # load files\n",
    "    raw_eegs = np.load(cfg.PRE_LOADED_RAW_EEGS, allow_pickle=True).item()\n",
    "    print(f\"Length of eegs: {len(raw_eegs)}\")\n",
    "\n",
    "    READ_SPEC_FILES = False\n",
    "    paths_spectrograms = glob.glob(cfg.TRAIN_SPECTOGRAMS + \"*.parquet\")\n",
    "    print(f'There are {len(paths_spectrograms)} spectrogram parquets')\n",
    "\n",
    "    all_spectrograms = np.load(cfg.PRE_LOADED_SPECTROGRAMS, allow_pickle=True).item()\n",
    "\n",
    "    READ_EEG_SPEC_FILES = False\n",
    "    paths_eegs = glob.glob(cfg.TRAIN_EEG_SPECTROGRAM + \"*.npy\")\n",
    "    print(f'There are {len(paths_eegs)} EEG spectrograms')\n",
    "\n",
    "    all_eegs = np.load(cfg.PRE_LOADED_EEG_SPECTROGRAMS, allow_pickle=True).item()\n",
    "\n",
    "    print(f\"Shape of Kaggle spectrograms: {all_spectrograms[train.loc[0,'spectrogram_id']].shape}\")\n",
    "    print(f\"Shape of EEG spectrograms: {all_eegs[train.loc[0,'eeg_id']].shape}\")\n",
    "    print(f\"Shape of raw eegs: {raw_eegs[train.loc[0,'eeg_id']].shape}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = FullDataset(train[train[\"fold\"] != ori_cfg.val_fold], \n",
    "                                eegs = raw_eegs,\n",
    "                                specs = all_spectrograms,\n",
    "                                eeg_specs = all_eegs,\n",
    "                                mode=\"train\")\n",
    "    val_dataset = FullDataset(train[train[\"fold\"] == ori_cfg.val_fold],\n",
    "                                eegs = raw_eegs,\n",
    "                                specs = all_spectrograms,\n",
    "                                eeg_specs = all_eegs,\n",
    "                                mode=\"val\")\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset, batch_size=ori_cfg.batch_size, num_workers=4, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=ori_cfg.batch_size, num_workers=16, shuffle=True, drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=ori_cfg.batch_size, num_workers=16, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivLossWithLogits(torch.nn.KLDivLoss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        y = nn.functional.log_softmax(y,  dim=1)\n",
    "        loss = super().forward(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm1d(8, affine = True)\n",
    "        self.norm1a = torch.nn.LayerNorm(8)\n",
    "        self.mha1 = torch.nn.MultiheadAttention(8, 4, dropout=0.1)\n",
    "        self.norm1b = torch.nn.LayerNorm(8)\n",
    "        self.nn1 = torch.nn.Linear(8, 8)\n",
    "\n",
    "        self.norm2a = torch.nn.LayerNorm(8)\n",
    "        self.mha2 = torch.nn.MultiheadAttention(8, 4, dropout=0.1)\n",
    "        self.norm2b = torch.nn.LayerNorm(8)\n",
    "        self.nn2 = torch.nn.Linear(8, 8)\n",
    "\n",
    "        self.norm3a = torch.nn.LayerNorm(8)\n",
    "        self.mha3 = torch.nn.MultiheadAttention(8, 4, dropout=0.1)\n",
    "        self.norm3b = torch.nn.LayerNorm(8)\n",
    "        self.nn3 = torch.nn.Linear(8, 8)\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(8)\n",
    "        self.mha3 = torch.nn.MultiheadAttention(8, 8, dropout=0.1)\n",
    "        self.gap = torch.nn.GlobalAveragePool1d()\n",
    "    \n",
    "        self.layer_norm2 = torch.nn.BatchNorm1d(10000)\n",
    "        self.nn5 = torch.nn.Linear(10000, 6)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        input = torch.transpose(input, 1,2)\n",
    "        input = self.instance_norm(input)\n",
    "        input = torch.transpose(input, 1,2)\n",
    "        x, attn_w = self.mha1(x, x, x)\n",
    "\n",
    "        x = self.norm1b(x + input)\n",
    "        x = self.nn1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x2 = input+x\n",
    "\n",
    "        x = self.norm2a(x2)\n",
    "        x, attn_w = self.mha2(x, x, x)\n",
    "        x = self.norm2b(x+x2)\n",
    "        x = self.nn2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x3 = x2+x\n",
    "\n",
    "        x = self.norm3a(x3)\n",
    "        x, attn_w = self.mha3(x, x, x)\n",
    "        x = self.norm3b(x+x3)\n",
    "        x = self.nn3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.layer_norm(x+input)\n",
    "        x= self.nn4(x)\n",
    "        # x = self.layer_norm2(x_n)\n",
    "        # x_a, attn_w = self.mha4(x, x, x)\n",
    "        # x = x_n+x_a \n",
    "        x = self.gap(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.nn5(x)\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ResNet_1D_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downsampling):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=False)\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=padding, bias=False)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "        identity = self.downsampling(x)\n",
    "\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "\n",
    "    def __init__(self, kernels, in_channels=8, fixed_kernel_size=17, num_classes=6):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.planes = 24\n",
    "        self.parallel_conv = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        for i, kernel_size in enumerate(list(self.kernels)):\n",
    "            sep_conv = nn.Conv1d(in_channels=in_channels, out_channels=self.planes, kernel_size=(kernel_size),\n",
    "                               stride=1, padding=0, bias=False,)\n",
    "            self.parallel_conv.append(sep_conv)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.planes, out_channels=self.planes, kernel_size=fixed_kernel_size,\n",
    "                               stride=2, padding=2, bias=False)\n",
    "        self.block = self._make_resnet_layer(kernel_size=fixed_kernel_size, stride=1, padding=fixed_kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "        self.rnn = Mamba(\n",
    "                        # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "                        d_model=self.in_channels, # Model dimension d_model\n",
    "                        d_state=128,  # SSM state expansion factor\n",
    "                        d_conv=4,    # Local convolution width\n",
    "                        expand=2,    # Block expansion factor\n",
    "                    )\n",
    "        # self.rnn = nn.GRU(input_size=self.in_channels, hidden_size=128, num_layers=1, bidirectional=True)\n",
    "        self.fc = nn.Linear(in_features=424, out_features=num_classes)\n",
    "        self.rnn1= Mamba(\n",
    "                        # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "                        d_model=156, # Model dimension d_model\n",
    "                        d_state=156,  # SSM state expansion factor\n",
    "                        d_conv=4,    # Local convolution width\n",
    "                        expand=2,    # Block expansion factor\n",
    "                    )\n",
    "        # self.rnn1 = nn.GRU(input_size=156, hidden_size=156, num_layers=1, bidirectional=True)\n",
    "\n",
    "    def _make_resnet_layer(self, kernel_size, stride, blocks=9, padding=0):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        base_width = self.planes\n",
    "\n",
    "        for i in range(blocks\n",
    "                    nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "                )\n",
    "            layers.append(ResNet_1D_Block(in_channels=self.planes, out_channels=self.planes, kernel_size=kernel_size,\n",
    "                                       stride=stride, padding=padding, downsampling=downsampling))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        out_sep = []\n",
    "\n",
    "        for i in range(len(self.kernels)):\n",
    "            sep = self.parallel_conv[i](x)\n",
    "            out_sep.append(sep)\n",
    "\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)  \n",
    "\n",
    "        out = self.block(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.avgpool(out)  \n",
    "        \n",
    "        out = out.reshape(out.shape[0], -1)  \n",
    "\n",
    "        rnn_out, _ = self.rnn(x.permute(0,2, 1))\n",
    "        new_rnn_h = rnn_out[:, -1, :]  \n",
    "\n",
    "        new_out = torch.cat([out, new_rnn_h], dim=1)  \n",
    "        result = self.fc(new_out)  \n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CustomModel\n",
    "model = CustomModel() # EEGNet(kernels=[1,5,10,20]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand(33, 10000, 8)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "class HMSLitModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 loss = KLDivLossWithLogits(),\n",
    "                 learning_rate_scheduler = None, \n",
    "                 weight_decay = 1e-6, # 1e-6\n",
    "                 max_epoch = 10, \n",
    "                #  log_tool = \"wandb\",\n",
    "                 cfg =None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "        # if self.log_tool  ==\"wandb\":\n",
    "        #     wandb.init(project='hms', config=dict(cfg))\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # x, y = batch[\"data\"], batch[\"target\"]\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # x, y = batch[\"data\"], batch[\"target\"]\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('val_loss', loss, on_epoch= True)\n",
    "        return loss\n",
    "\n",
    "    # def validation_epoch_end(self, batch, outs):\n",
    "    #     # outs is a list of whatever you returned in `validation_step`\n",
    "    #     loss = torch.stack(outs).mean()\n",
    "    #     self.log(\"val_loss\", loss)\n",
    "    # def on_train_epoch_end(self):\n",
    "        # wandb.log({\"train_loss\": loss})\n",
    "    #     self.train_acc.reset()\n",
    "\n",
    "    # def on_validation_epoch_end(self, outputs):\n",
    "    #     self.log('valid_acc_epoch', self.valid_acc.compute())\n",
    "    #     self.valid_acc.reset()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-4)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=3, max_epochs=50)\n",
    "        sch_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"name\": \"my_little_scheduler\",\n",
    "\t    }\n",
    "        return [optimizer], [sch_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.0734377 ,  6.114375  , -3.79      , ..., -1.0625    ,\n",
       "         -2.5215626 ,  4.0965624 ],\n",
       "        [-1.1637497 ,  6.4015627 , -3.6490622 , ..., -1.0565624 ,\n",
       "         -1.7603121 ,  3.8553123 ],\n",
       "        [-0.27875042,  6.2737494 , -2.7575006 , ..., -1.1981249 ,\n",
       "         -2.059062  ,  3.8156252 ],\n",
       "        ...,\n",
       "        [-2.5265622 ,  1.4449997 , -4.10625   , ..., -1.5796874 ,\n",
       "         -1.4046874 , -0.3303125 ],\n",
       "        [-2.85375   ,  1.9796875 , -3.814375  , ..., -1.1553125 ,\n",
       "         -1.6774998 ,  0.45656252],\n",
       "        [-2.5299997 ,  1.69875   , -4.0409374 , ..., -1.0615625 ,\n",
       "         -1.265625  , -0.08374977]], dtype=float32),\n",
       " tensor([0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.6667]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:94: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: Checkpoint directory checkpoints/ exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/tmp/ipykernel_79240/477768088.py:50: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=3, max_epochs=50)\n",
      "\n",
      "  | Name  | Type                | Params\n",
      "----------------------------------------------\n",
      "0 | model | CustomModel         | 98.1 K\n",
      "1 | loss  | KLDivLossWithLogits | 0     \n",
      "----------------------------------------------\n",
      "98.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.1 K    Total params\n",
      "0.392     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▍        | 50/348 [02:29<14:50,  2.99s/it, loss=1.31]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meru\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240315_120119-eh5x5k4u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eru/lightning_logs/runs/eh5x5k4u' target=\"_blank\">glamorous-planet-1</a></strong> to <a href='https://wandb.ai/eru/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eru/lightning_logs' target=\"_blank\">https://wandb.ai/eru/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eru/lightning_logs/runs/eh5x5k4u' target=\"_blank\">https://wandb.ai/eru/lightning_logs/runs/eh5x5k4u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  78%|███████▊  | 272/348 [18:09<05:04,  4.01s/it, loss=0.748, v_num=5k4u]"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(log_model=\"all\")\n",
    "model.to(device)\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "trainer = pl.Trainer(callbacks=[checkpoint_callback], logger=wandb_logger, accelerator='gpu', devices=1)\n",
    "hms_module = HMSLitModule(model = model.cuda(), cfg= cfg)\n",
    "trainer.fit(\n",
    "    hms_module, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=val_loader,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.1546876 , -2.8556252 ,  3.41125   , ..., -2.00875   ,\n",
       "          4.171875  , -3.6334374 ],\n",
       "        [ 1.6018751 , -3.4728124 ,  1.9696875 , ..., -0.13656247,\n",
       "          2.5865624 , -1.243125  ],\n",
       "        [ 0.71343756, -3.2353125 ,  1.09375   , ...,  0.11843753,\n",
       "          2.5190625 , -1.3634374 ],\n",
       "        ...,\n",
       "        [ 3.9434376 , -3.8453124 ,  3.7371874 , ..., -1.1993749 ,\n",
       "          4.0453124 , -2.4271874 ],\n",
       "        [ 4.4621873 , -3.57625   ,  4.231875  , ..., -1.4131249 ,\n",
       "          4.385937  , -2.69875   ],\n",
       "        [ 2.479375  , -3.753125  ,  1.6125    , ...,  0.541875  ,\n",
       "          2.9896874 , -0.429375  ]], dtype=float32),\n",
       " tensor([ 0.,  0.,  0.,  0.,  0., 30.]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmc-snuh-QMWTtYeW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
