{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEEGDatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HMSDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[0;32m~/HMC-SNUH/EEGDatasets.py:30\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mLoad EEG dataset from kaggle like below:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/torch/utils/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_osp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthroughput_benchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThroughputBenchmark\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rename_privateuse1_backend, generate_methods_for_privateuse1_backend\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/torch/utils/throughput_benchmark.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_time\u001b[39m(time_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, time_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, time_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Define time formatting.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "from EEGDatasets import HMSDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmsdataset = HMSDataset(train_grouped_eeg, parquet_file)\n",
    "dataloader = DataLoader(hmsdataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "target_p = [\n",
    "    'seizure_vote', \n",
    "    'lpd_vote', \n",
    "    'gpd_vote', \n",
    "    'lrda_vote', \n",
    "    'grda_vote', \n",
    "    'other_vote'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/data/hms/train.csv'\n",
    "parquet_file = '/data/hms/train_eegs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th>consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1825637311</td>\n",
       "      <td>20654</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1722186807</td>\n",
       "      <td>20230</td>\n",
       "      <td>LPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>154</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14960202</td>\n",
       "      <td>12</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>3254468733</td>\n",
       "      <td>5955</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>618728447</td>\n",
       "      <td>4</td>\n",
       "      <td>908.0</td>\n",
       "      <td>2898467035</td>\n",
       "      <td>38549</td>\n",
       "      <td>GPD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3255875127</td>\n",
       "      <td>40955</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17084</th>\n",
       "      <td>4293354003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1188113564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>447244163</td>\n",
       "      <td>16610</td>\n",
       "      <td>GRDA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17085</th>\n",
       "      <td>4293843368</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1549502620</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1618953053</td>\n",
       "      <td>15065</td>\n",
       "      <td>GRDA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17086</th>\n",
       "      <td>4294455489</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2105480289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>469526364</td>\n",
       "      <td>56</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>4294858825</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>657299228</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>561576493</td>\n",
       "      <td>4312</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>4294958358</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260520016</td>\n",
       "      <td>9</td>\n",
       "      <td>2508.0</td>\n",
       "      <td>2788887007</td>\n",
       "      <td>25986</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17089 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n",
       "0          568657           4                       0.0       789577333   \n",
       "1          582999          11                       0.0      1552638400   \n",
       "2          642382           2                       0.0        14960202   \n",
       "3          751790           1                       0.0       618728447   \n",
       "4          778705           1                       0.0        52296320   \n",
       "...           ...         ...                       ...             ...   \n",
       "17084  4293354003           1                       0.0      1188113564   \n",
       "17085  4293843368           1                       0.0      1549502620   \n",
       "17086  4294455489           1                       0.0      2105480289   \n",
       "17087  4294858825           5                       0.0       657299228   \n",
       "17088  4294958358           1                       0.0       260520016   \n",
       "\n",
       "       spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n",
       "0                       0                               0.0  1825637311   \n",
       "1                       0                               0.0  1722186807   \n",
       "2                      12                            1008.0  3254468733   \n",
       "3                       4                             908.0  2898467035   \n",
       "4                       0                               0.0  3255875127   \n",
       "...                   ...                               ...         ...   \n",
       "17084                   0                               0.0   447244163   \n",
       "17085                   0                               0.0  1618953053   \n",
       "17086                   0                               0.0   469526364   \n",
       "17087                   0                               0.0   561576493   \n",
       "17088                   9                            2508.0  2788887007   \n",
       "\n",
       "       patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  \\\n",
       "0           20654            Other           0.0  0.000000      0.25   \n",
       "1           20230              LPD           0.0  0.857143      0.00   \n",
       "2            5955            Other           0.0  0.000000      0.00   \n",
       "3           38549              GPD           0.0  0.000000      1.00   \n",
       "4           40955            Other           0.0  0.000000      0.00   \n",
       "...           ...              ...           ...       ...       ...   \n",
       "17084       16610             GRDA           0.0  0.000000      0.00   \n",
       "17085       15065             GRDA           0.0  0.000000      0.00   \n",
       "17086          56            Other           0.0  0.000000      0.00   \n",
       "17087        4312            Other           0.0  0.000000      0.00   \n",
       "17088       25986            Other           0.0  0.000000      0.00   \n",
       "\n",
       "       lrda_vote  grda_vote  other_vote  total_evaluators  consensus  \n",
       "0       0.000000   0.166667    0.583333                48         28  \n",
       "1       0.071429   0.000000    0.071429               154        132  \n",
       "2       0.000000   0.000000    1.000000                 2          2  \n",
       "3       0.000000   0.000000    0.000000                 1          1  \n",
       "4       0.000000   0.000000    1.000000                 2          2  \n",
       "...          ...        ...         ...               ...        ...  \n",
       "17084   0.000000   0.500000    0.500000                 2          1  \n",
       "17085   0.000000   0.500000    0.500000                 2          1  \n",
       "17086   0.000000   0.000000    1.000000                 1          1  \n",
       "17087   0.000000   0.066667    0.933333                75         70  \n",
       "17088   0.000000   0.000000    1.000000                 2          2  \n",
       "\n",
       "[17089 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(train_file)\n",
    "train['total_evaluators'] = train[target_p].sum(axis=1)\n",
    "\n",
    "\n",
    "train_grouped_eeg = train.groupby('eeg_id').agg({\n",
    "    'eeg_sub_id': 'count',  # 计算每个 eeg_id 对应的行数\n",
    "    'eeg_label_offset_seconds': 'first',\n",
    "    'spectrogram_id': 'first',\n",
    "    'spectrogram_sub_id': 'first',\n",
    "    'spectrogram_label_offset_seconds': 'first',\n",
    "    'label_id': 'first',\n",
    "    'patient_id': 'first',\n",
    "    'expert_consensus': 'first',\n",
    "    'seizure_vote': 'sum',\n",
    "    'lpd_vote': 'sum',\n",
    "    'gpd_vote': 'sum',\n",
    "    'lrda_vote': 'sum',\n",
    "    'grda_vote': 'sum',\n",
    "    'other_vote': 'sum',\n",
    "    'total_evaluators': 'sum'\n",
    "}).reset_index()\n",
    "    \n",
    "train_grouped_eeg['consensus'] = train_grouped_eeg[target_p].max(axis=1)\n",
    "\n",
    "# 计算每一列的比例\n",
    "train_grouped_eeg[target_p] = train_grouped_eeg[target_p].div(\n",
    "    train_grouped_eeg['total_evaluators'], \n",
    "    axis=0\n",
    ")\n",
    "    \n",
    "train_grouped_eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmsdataset = HMSDataset(train_grouped_eeg, parquet_file)\n",
    "dataloader = DataLoader(hmsdataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMInceptionV3\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape, num_class):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(MInceptionV3, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, y_hat, y):\n",
    "        y_hat = F.log_softmax(y_hat, dim=-1)\n",
    "        return -torch.sum(y * torch.log(y_hat + 1e-10)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(EEGDataset(df, eeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(df, eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PatchTSTConfig, PatchTSTForClassification\n",
    "\n",
    "# classification task with two input channel2 and 3 classes\n",
    "config = PatchTSTConfig(\n",
    "    num_input_channels=8,\n",
    "    num_targets=6,\n",
    "    context_length=512,\n",
    "    patch_length=12,\n",
    "    stride=12,\n",
    "    use_cls_token=True,\n",
    ")\n",
    "model = PatchTSTForClassification(config=config)\n",
    "\n",
    "# during inference, one only provides past values\n",
    "# past_values = torch.randn(20, 512, 2)\n",
    "# outputs = model(past_values=past_values)\n",
    "# labels = outputs.prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=128, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ones_like(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X_batch)\n\u001b[0;32m----> 9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/transformers/models/patchtst/modeling_patchtst.py:1518\u001b[0m, in \u001b[0;36mPatchTSTForClassification.forward\u001b[0;34m(self, past_values, target_values, past_observed_mask, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m    past_values (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`, *required*):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m>>> labels = outputs.prediction_logits\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1518\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_observed_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1525\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(model_output\u001b[38;5;241m.\u001b[39mlast_hidden_state)\n\u001b[1;32m   1527\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/hmc-snuh-QMWTtYeW-py3.10/lib/python3.10/site-packages/transformers/models/patchtst/modeling_patchtst.py:1231\u001b[0m, in \u001b[0;36mPatchTSTModel.forward\u001b[0;34m(self, past_values, past_observed_mask, future_values, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   1226\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1227\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1228\u001b[0m )\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_observed_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1231\u001b[0m     past_observed_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;66;03m# x: tensor [bs x sequence_length x num_input_channels]\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m scaled_past_values, loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler(past_values, past_observed_mask)\n",
      "\u001b[0;31mTypeError\u001b[0m: ones_like(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    # Training\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"test_trainer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmc-snuh-QMWTtYeW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
